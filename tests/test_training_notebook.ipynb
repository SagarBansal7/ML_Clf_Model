{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c8cabf0-99c7-4216-9e33-b5fbc91ec5e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.run('train_model_py', 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "782265a2-07ad-4684-a6ff-99d2cd5fb735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pytest\n",
    "# !pip install mlflow\n",
    "import sys\n",
    "sys.path.insert(0, '/Workspace/Users/sagarbansal719@gmail.com/ML_Clf_Model/notebooks')\n",
    "\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from unittest.mock import MagicMock\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from train_model_py import WineDataProcessor, WineQualityModel, MLflowExperiment, FeatureImportance\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Mock Data\n",
    "@pytest.fixture\n",
    "def mock_data():\n",
    "    \"\"\"Fixture to provide mock data for training/testing.\"\"\"\n",
    "    processor = WineDataProcessor()\n",
    "    data = processor.load_data()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = processor.split_data()\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_model():\n",
    "    \"\"\"Fixture to provide a mock model for testing.\"\"\"\n",
    "    model = WineQualityModel(RandomForestClassifier(n_estimators=10, random_state=123))\n",
    "    return model\n",
    "\n",
    "def test_data_loading(mock_data):\n",
    "    \"\"\"Test that the data loads and splits correctly.\"\"\"\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = mock_data\n",
    "    \n",
    "    assert isinstance(X_train, pd.DataFrame), \"X_train should be a pandas DataFrame\"\n",
    "    assert X_train.shape[0] > 0, \"Training data should have samples\"\n",
    "    assert len(y_train) == X_train.shape[0], \"X_train and y_train should have the same number of samples\"\n",
    "\n",
    "def test_model_training(mock_data, mock_model):\n",
    "    \"\"\"Test the model training process.\"\"\"\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = mock_data\n",
    "    mock_model.train(X_train, y_train)\n",
    "    \n",
    "    # Ensure the model is trained (it should have learned parameters)\n",
    "    assert hasattr(mock_model.model, 'n_estimators'), \"Model should have a trained classifier\"\n",
    "    assert mock_model.model.n_estimators == 10, \"RandomForestClassifier should have 10 estimators\"\n",
    "\n",
    "def test_model_evaluation(mock_data, mock_model):\n",
    "    \"\"\"Test the model evaluation process.\"\"\"\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = mock_data\n",
    "    mock_model.train(X_train, y_train)\n",
    "    \n",
    "    auc_score = mock_model.evaluate(X_test, y_test)\n",
    "    \n",
    "    assert isinstance(auc_score, float), \"AUC score should be a float\"\n",
    "    assert 0 <= auc_score <= 1, \"AUC score should be between 0 and 1\"\n",
    "\n",
    "def test_mlflow_logging(mock_data, mock_model):\n",
    "    \"\"\"Test that the model is logged correctly to MLflow.\"\"\"\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = mock_data\n",
    "    experiment = MLflowExperiment(model_name=\"wine_quality_test\")\n",
    "    \n",
    "    # Mock MLflow's start_run to avoid actual logging\n",
    "    with pytest.monkeypatch.context() as mp:\n",
    "        mp.setattr(mlflow, 'start_run', MagicMock())\n",
    "        \n",
    "        # Run experiment & log model\n",
    "        run_id = experiment.run_experiment(mock_model, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Check that MLflow's start_run was called\n",
    "        mlflow.start_run.assert_called_once()\n",
    "        assert run_id is not None, \"Run ID should be returned from the experiment\"\n",
    "\n",
    "def test_feature_importance(mock_data, mock_model):\n",
    "    \"\"\"Test feature importance extraction.\"\"\"\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = mock_data\n",
    "    mock_model.train(X_train, y_train)\n",
    "    \n",
    "    feature_importance = FeatureImportance.get_importance(mock_model, X_train)\n",
    "    \n",
    "    assert isinstance(feature_importance, pd.DataFrame), \"Feature importance should be a pandas DataFrame\"\n",
    "    assert 'importance' in feature_importance.columns, \"Feature importance DataFrame should contain 'importance' column\"\n",
    "    assert feature_importance.shape[0] == X_train.shape[1], \"Feature importance should match the number of features\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "test_training_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
