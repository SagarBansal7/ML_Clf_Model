{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c91a007-3f5e-4195-b6bb-9948440013fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: Model Inference\n",
    "\n",
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"LoanInference\").getOrCreate()\n",
    "\n",
    "# Load the latest version of the registered model\n",
    "model_name = \"loan_default_model\"\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{model_name}/latest\")\n",
    "\n",
    "# Load new data for inference\n",
    "df = spark.read.parquet(\"/databricks-datasets/samples/lending_club/parquet/\").toPandas()\n",
    "df = df[[\"loan_amnt\", \"funded_amnt\", \"term\", \"int_rate\", \"installment\", \"annual_inc\", \"dti\", \"delinq_2yrs\"]]\n",
    "df[\"term\"] = df[\"term\"].str.replace(\" months\", \"\").astype(int)\n",
    "\n",
    "# Select 5 random samples for inference\n",
    "X_new = df.sample(5, random_state=42)\n",
    "\n",
    "# Run Inference\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Save results\n",
    "results = X_new.copy()\n",
    "results[\"prediction\"] = predictions\n",
    "display(results)  # Show results in Databricks\n",
    "results.to_csv(\"/dbfs/ml/inference_results.csv\", index=False)\n",
    "\n",
    "print(\"✅ Inference completed! Results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f44aea9-e078-40c5-afc3-7b0669f83f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: Model Inference\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"LoanInference\").getOrCreate()\n",
    "\n",
    "# Step 2: Load the Trained Model from MLflow Model Registry\n",
    "model_name = \"loan_model5\"\n",
    "model_version = 2\n",
    "\n",
    "# Workaround to set the registry URI manually\n",
    "#mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = lambda: \"databricks-uc\"\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Download the model artifacts\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "destination_path = \"/tmp/model\"\n",
    "mlflow.artifacts.download_artifacts(artifact_uri=model_uri, dst_path=destination_path)\n",
    "\n",
    "# Load the model from the Databricks Model Registry\n",
    "#model = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\n",
    "\n",
    "# Load the model from the downloaded artifacts\n",
    "model = mlflow.sklearn.load_model(destination_path)\n",
    "\n",
    "# Step 3: Load New Data\n",
    "# Simulating new loan applications from Databricks sample dataset\n",
    "df_new = spark.read.parquet(\"/databricks-datasets/samples/lending_club/parquet/\")\n",
    "\n",
    "# Step 4: Save New Data to a Table\n",
    "df_new.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"new_loan_data\")\n",
    "\n",
    "# Step 5: Load the Data Back from the Table\n",
    "df_table = spark.read.table(\"new_loan_data\")\n",
    "\n",
    "# Step 6: Convert the DataFrame to Pandas for Processing\n",
    "df_pandas = df_table.toPandas()\n",
    "\n",
    "# Step 7: Feature Engineering (same steps as training)\n",
    "df_pandas = df_pandas.dropna(subset=['loan_amnt', 'funded_amnt', 'int_rate', 'installment', 'annual_inc', 'dti', 'delinq_2yrs'])\n",
    "\n",
    "# Select relevant columns\n",
    "df_pandas = df_pandas[[\"loan_amnt\", \"funded_amnt\", \"term\", \"int_rate\", \"installment\", \"annual_inc\", \"dti\", \"delinq_2yrs\"]]\n",
    "\n",
    "# Convert categorical column\n",
    "df_pandas[\"term\"] = df_pandas[\"term\"].str.replace(\" months\", \"\").astype(int)\n",
    "\n",
    "# Convert string percentages to floats\n",
    "df_pandas['int_rate'] = df_pandas['int_rate'].str.replace('%', '').astype(float)\n",
    "\n",
    "# Standardize the input features (using the same scaler as training)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_pandas)\n",
    "\n",
    "# Step 8: Perform Inference\n",
    "predictions = model.predict(X_scaled)\n",
    "\n",
    "# Step 9: Convert Predictions to DataFrame\n",
    "df_pandas[\"loan_default_prediction\"] = predictions\n",
    "\n",
    "# Convert Pandas DataFrame back to Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Step 10: Save Inference Results to a Table\n",
    "df_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"loan_inference_results\")\n",
    "\n",
    "print(\"✅ Inference completed and results saved to table 'loan_inference_results'.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "run_inference.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
